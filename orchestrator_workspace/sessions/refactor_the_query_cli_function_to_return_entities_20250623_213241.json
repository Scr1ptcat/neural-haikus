{
  "id": "refactor_the_query_cli_function_to_return_entities_20250623_213241",
  "feature_request": "Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches.",
  "timestamp": "20250623_213241",
  "status": "implementing",
  "project_type": null,
  "files": {
    "orchestrator_prompt": "# Role: Senior Technical Architect & Project Planner\n\nYou are a senior technical architect with 20+ years of experience in software development, system design, and project planning. You excel at:\n- Rapidly understanding codebases and their patterns\n- Breaking down vague requirements into concrete technical specifications  \n- Creating detailed implementation plans that junior developers can follow\n- Anticipating integration challenges and edge cases\n- Estimating effort accurately based on project complexity\n\n## Your Current Task\n\nYou've been asked to prepare comprehensive work sessions for implementing a new feature. You need to:\n1. Quickly analyze the current project to understand its architecture\n2. Interpret the feature request into detailed technical requirements\n3. Create filled templates that another developer (or AI) can execute without additional context\n\n## Project Analysis Phase\n\nTake 30-60 seconds to scan the project:\n\n```bash\n# Quick discovery commands to understand the project\nfind . -name \"package.json\" -o -name \"requirements.txt\" -o -name \"go.mod\" -o -name \"Gemfile\" | head -5\nfind . -type f -name \"*.py\" -o -name \"*.js\" -o -name \"*.ts\" -o -name \"*.go\" | grep -E \"(main|app|index|server)\" | head -10\nls -la\ntree -d -L 2 -I 'node_modules|__pycache__|.git'\ngrep -r \"class.*Controller\\|router\\.|@app.route\\|@Get\\|@Post\" . --include=\"*.py\" --include=\"*.js\" --include=\"*.ts\" | head -10\n```\n\n## Feature Request: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches.\n\n## Required Output\n\nYou must provide THREE comprehensive sections:\n\n### 1. ANALYSIS_TEMPLATE\n```\n# Role: Senior Software Analyst\n\nYou are a senior software analyst with expertise in code archaeology and pattern recognition. Your mission is to perform a deep analysis of this codebase to understand every aspect that will impact the implementation of: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches.\n\n## Analysis Objectives\n1. Map the complete architecture and understand design decisions\n2. Identify all patterns, conventions, and standards\n3. Locate integration points for the new feature\n4. Understand the testing philosophy and approach\n5. Document everything needed for successful implementation\n\n## Phase 1: Project Structure Deep Dive\n[Specific areas to investigate based on the feature]\n- Directory organization and what belongs where\n- File naming conventions and patterns\n- Module organization and dependencies\n- Build and deployment structure\n\n## Phase 2: Architecture Analysis\n[What architectural aspects to focus on]\n- Design patterns in use (MVC, microservices, etc.)\n- State management approach\n- Data flow patterns\n- External service integrations\n- Middleware/plugin architecture\n\n## Phase 3: Code Standards Discovery\n[Specific patterns to document]\n- Naming conventions for files, functions, variables, classes\n- Code organization within files\n- Import/export patterns\n- Error handling approaches\n- Logging standards\n- Comment and documentation style\n\n## Phase 4: Feature-Specific Investigation\n[Targeted analysis for this feature]\n- Similar existing features to use as reference\n- Specific integration points needed\n- Data models that will be affected\n- API endpoints or interfaces to modify/create\n- Security considerations\n\n## Phase 5: Testing Infrastructure\n[Testing approach analysis]\n- Test file organization\n- Testing frameworks and patterns\n- Mock/stub approaches\n- Test data management\n- Coverage expectations\n\n## Expected Deliverable\nA comprehensive analysis document that allows someone to implement Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches. without needing to ask any questions about the codebase.\n```\n\n### 2. IMPLEMENTATION_TEMPLATE\n```\n# Role: Senior Software Engineer\n\nYou are a senior software engineer with deep expertise in building production-ready features. You have:\n- 15+ years of experience in software development\n- Expertise in maintaining code quality and consistency\n- Strong focus on testing and documentation\n- Ability to implement complex features while following existing patterns perfectly\n\n## Your Mission\nImplement the feature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches.\n\nYou must follow the patterns and conventions discovered during the analysis phase EXACTLY. No improvements or refactoring - pure feature implementation following existing patterns.\n\n## Implementation Constraints\n- Follow existing patterns precisely\n- Minimize changes to existing code\n- Write comprehensive tests\n- Document all decisions\n- Handle errors consistently with the project\n- Maintain backward compatibility\n\n## Phase 1: Pre-Implementation Setup\n- Review analysis findings\n- Set up development environment\n- Verify all dependencies\n- Create feature branch (if using git)\n\n## Phase 2: Implementation Planning\n[Detailed breakdown based on feature requirements]\n- Component breakdown with rationale\n- File creation/modification list\n- Integration points and approach\n- Data model changes (if any)\n- API changes (if any)\n\n## Phase 3: Core Implementation\n[Step-by-step implementation guide]\n1. Data Layer (if applicable)\n   - Schema changes\n   - Migrations\n   - Model updates\n   \n2. Business Logic\n   - Core feature logic\n   - Validation rules\n   - Business constraints\n   - Error handling\n   \n3. Interface Layer\n   - API endpoints\n   - UI components\n   - CLI commands\n   - External interfaces\n\n4. Integration\n   - Connect all components\n   - Update existing flows\n   - Maintain compatibility\n\n## Phase 4: Testing Implementation\n[Comprehensive testing requirements]\n- Unit tests for all new functions/methods\n- Integration tests for component interactions\n- End-to-end tests for user flows\n- Edge case testing\n- Error scenario testing\n- Performance testing (if applicable)\n\n## Phase 5: Documentation and Polish\n- Code documentation (inline comments, docstrings)\n- API documentation updates\n- User documentation (if applicable)\n- Configuration documentation\n- Migration guide (if breaking changes)\n\n## Expected Deliverable\nComplete, working implementation of Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches. that:\n- Follows all existing patterns\n- Includes comprehensive tests\n- Is properly documented\n- Handles all edge cases\n- Integrates seamlessly with existing code\n```\n\n### 3. EXECUTION_NOTES\n```\nFeature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches.\nComplexity: [Low/Medium/High based on analysis]\nEstimated Time: [X-Y hours broken down by phase]\n  - Analysis: [X hours]\n  - Implementation: [Y hours]\n  - Testing: [Z hours]\n  - Documentation: [W hours]\n\nKey Risks:\n[Identified risks with mitigation strategies]\n\nKey Files to Focus On:\n[List of critical files for this feature]\n\nSimilar Existing Features:\n[Reference implementations to follow]\n\nIntegration Points:\n[Where this feature touches existing code]\n\nTesting Strategy:\n[Specific testing approach for this feature]\n\nPerformance Considerations:\n[Any performance impacts to consider]\n\nSecurity Considerations:\n[Security aspects to address]\n\nRollback Plan:\n[How to rollback if issues arise]\n```\n\n## Important Notes\n\n1. Be extremely specific in your templates - someone should be able to execute them without any project knowledge\n2. Include actual file paths, function names, and patterns you observe\n3. Don't make assumptions - base everything on what you can see in the project\n4. If something is unclear, make a reasonable inference and document it\n5. Always err on the side of being too detailed rather than too vague\n\nRemember: The goal is to reduce the implementation time from hours of exploration to minutes of execution.",
    "analysis_template": "2. Identify all patterns, conventions, and standards\n  3. Locate integration points for the new feature\n  4. Understand the testing philosophy and approach\n  5. Document everything needed for successful implementation\n\n  ## Phase 1: Project Structure Deep Dive\n  [Specific areas to investigate based on the feature]\n  - CLI command organization in `airules/cli/commands/`\n  - Delta Spark models in `airules/delta_spark/models/`\n  - Table registry and schema management in `airules/delta_spark/`\n  - Current query command implementation in `airules/cli/commands/query.py`\n  - Infrastructure commands in `airules/cli/commands/infrastructure.py` for display patterns\n\n  ## Phase 2: Architecture Analysis\n  [What architectural aspects to focus on]\n  - Click framework patterns with AppContext passing\n  - Delta Lake integration with AbstractDeltaModel base class\n  - Entity relationships: Docket \u2192 Documents \u2192 Comments \u2192 Attachments\n  - QueryBuilder and QueryResultsHandler patterns in current query command\n  - DeltaTableRegistry for table management\n  - Configuration system in `airules/config.py`\n  - Spark session management in `airules/utils/spark_session.py`\n\n  ## Phase 3: Code Standards Discovery\n  [Specific patterns to document]\n  - Command naming: verb-noun pattern (e.g., list-tables, show-config)\n  - CLI decorator pattern: @click.command() with @pass_context\n  - Error handling: try/except with click.echo for user messages\n  - Progress bars: tqdm for long operations\n  - Output formatting: support for table (default), json, csv\n  - Class naming: PascalCase with descriptive suffixes (Handler, Builder, Manager)\n  - Method naming: snake_case with clear action verbs\n  - Import organization: standard library, third-party, local imports\n\n  ## Phase 4: Feature-Specific Investigation\n  [Targeted analysis for this feature]\n  - Current query command limitations (only queries comments)\n  - Entity model methods to leverage:\n    - Docket.get_by_id(), Document.get_by_docket()\n    - Comment.get_by_document(), Attachment.get_by_comment()\n    - Comment.get_comments_with_attachments()\n  - Display patterns from infrastructure commands:\n    - Table formatting with headers and separators\n    - JSON output with proper serialization\n    - CSV export capabilities\n  - Configurable switches pattern from existing commands (--format, --output-file)\n  - Entity type detection from table names in DeltaTableRegistry\n\n  ## Phase 5: Testing Infrastructure\n  [Testing approach analysis]\n  - Test organization in `tests/unit/` and `tests/integration/`\n  - Mock patterns using unittest.mock\n  - Test CLI commands with Click's CliRunner\n  - Test data fixtures and sample docket IDs\n  - Delta table testing with temporary tables\n  - Coverage expectations from existing test suites\n\n  ## Expected Deliverable\n  A comprehensive analysis document that allows someone to implement Refactor the query CLI function to return entities against their route. so if its query docket\n  {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches. without needing to ask any questions about the codebase.\n\n  Key findings should include:\n  - Exact patterns for CLI command implementation\n  - Entity traversal logic for hierarchical data retrieval\n  - Display formatting patterns to maintain consistency\n  - Configuration and switch implementation patterns\n  - Testing patterns for CLI commands with Delta Lake integration\n\n\u25cf Update Todos\n  \u23bf \u00a0\u2612 Create detailed analysis template for understanding AIRules architecture\n     \u2610 Create implementation template for refactoring query command\n     \u2610 Create execution notes with risk assessment and estimates\n\n\u25cf IMPLEMENTATION_TEMPLATE\n\n  # Role: Senior Software Engineer\n\n  You are a senior software engineer with deep expertise in building production-ready features. You have:\n  - 15+ years of experience in software development\n  - Expertise in maintaining code quality and consistency\n  - Strong focus on testing and documentation\n  - Ability to implement complex features while following existing patterns perfectly\n\n  ## Your Mission\n  Implement the feature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its\n  documents, comments, attachments, etc, with configurable switches.\n\n  You must follow the patterns and conventions discovered during the analysis phase EXACTLY. No improvements or refactoring - pure feature implementation following existing\n   patterns.\n\n  ## Implementation Constraints\n  - Follow existing patterns precisely\n  - Minimize changes to existing code\n  - Write comprehensive tests\n  - Document all decisions\n  - Handle errors consistently with the project\n  - Maintain backward compatibility\n\n  ## Phase 1: Pre-Implementation Setup\n  - Review analysis findings\n  - Set up development environment with AIRules\n  - Verify Spark and Delta Lake are working\n  - Create feature branch: `git checkout -b feature/unified-query-command`\n  - Run existing tests to ensure baseline: `./tests/run_all_tests.sh`\n\n  ## Phase 2: Implementation Planning\n  [Detailed breakdown based on feature requirements]\n\n  1. **Refactor existing query.py command**\n     - Keep existing comment-specific functionality as legacy support\n     - Extract common query patterns into reusable components\n     - File: `airules/cli/commands/query.py`",
    "implementation_template": "3. Locate integration points for the new feature\n  4. Understand the testing philosophy and approach\n  5. Document everything needed for successful implementation\n\n  ## Phase 1: Project Structure Deep Dive\n  [Specific areas to investigate based on the feature]\n  - CLI command organization in `airules/cli/commands/`\n  - Delta Spark models in `airules/delta_spark/models/`\n  - Table registry and schema management in `airules/delta_spark/`\n  - Current query command implementation in `airules/cli/commands/query.py`\n  - Infrastructure commands in `airules/cli/commands/infrastructure.py` for display patterns\n\n  ## Phase 2: Architecture Analysis\n  [What architectural aspects to focus on]\n  - Click framework patterns with AppContext passing\n  - Delta Lake integration with AbstractDeltaModel base class\n  - Entity relationships: Docket \u2192 Documents \u2192 Comments \u2192 Attachments\n  - QueryBuilder and QueryResultsHandler patterns in current query command\n  - DeltaTableRegistry for table management\n  - Configuration system in `airules/config.py`\n  - Spark session management in `airules/utils/spark_session.py`\n\n  ## Phase 3: Code Standards Discovery\n  [Specific patterns to document]\n  - Command naming: verb-noun pattern (e.g., list-tables, show-config)\n  - CLI decorator pattern: @click.command() with @pass_context\n  - Error handling: try/except with click.echo for user messages\n  - Progress bars: tqdm for long operations\n  - Output formatting: support for table (default), json, csv\n  - Class naming: PascalCase with descriptive suffixes (Handler, Builder, Manager)\n  - Method naming: snake_case with clear action verbs\n  - Import organization: standard library, third-party, local imports\n\n  ## Phase 4: Feature-Specific Investigation\n  [Targeted analysis for this feature]\n  - Current query command limitations (only queries comments)\n  - Entity model methods to leverage:\n    - Docket.get_by_id(), Document.get_by_docket()\n    - Comment.get_by_document(), Attachment.get_by_comment()\n    - Comment.get_comments_with_attachments()\n  - Display patterns from infrastructure commands:\n    - Table formatting with headers and separators\n    - JSON output with proper serialization\n    - CSV export capabilities\n  - Configurable switches pattern from existing commands (--format, --output-file)\n  - Entity type detection from table names in DeltaTableRegistry\n\n  ## Phase 5: Testing Infrastructure\n  [Testing approach analysis]\n  - Test organization in `tests/unit/` and `tests/integration/`\n  - Mock patterns using unittest.mock\n  - Test CLI commands with Click's CliRunner\n  - Test data fixtures and sample docket IDs\n  - Delta table testing with temporary tables\n  - Coverage expectations from existing test suites\n\n  ## Expected Deliverable\n  A comprehensive analysis document that allows someone to implement Refactor the query CLI function to return entities against their route. so if its query docket\n  {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches. without needing to ask any questions about the codebase.\n\n  Key findings should include:\n  - Exact patterns for CLI command implementation\n  - Entity traversal logic for hierarchical data retrieval\n  - Display formatting patterns to maintain consistency\n  - Configuration and switch implementation patterns\n  - Testing patterns for CLI commands with Delta Lake integration\n\n\u25cf Update Todos\n  \u23bf \u00a0\u2612 Create detailed analysis template for understanding AIRules architecture\n     \u2610 Create implementation template for refactoring query command\n     \u2610 Create execution notes with risk assessment and estimates\n\n\u25cf IMPLEMENTATION_TEMPLATE\n\n  # Role: Senior Software Engineer\n\n  You are a senior software engineer with deep expertise in building production-ready features. You have:\n  - 15+ years of experience in software development\n  - Expertise in maintaining code quality and consistency\n  - Strong focus on testing and documentation\n  - Ability to implement complex features while following existing patterns perfectly\n\n  ## Your Mission\n  Implement the feature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its\n  documents, comments, attachments, etc, with configurable switches.\n\n  You must follow the patterns and conventions discovered during the analysis phase EXACTLY. No improvements or refactoring - pure feature implementation following existing\n   patterns.\n\n  ## Implementation Constraints\n  - Follow existing patterns precisely\n  - Minimize changes to existing code\n  - Write comprehensive tests\n  - Document all decisions\n  - Handle errors consistently with the project\n  - Maintain backward compatibility\n\n  ## Phase 1: Pre-Implementation Setup\n  - Review analysis findings\n  - Set up development environment with AIRules\n  - Verify Spark and Delta Lake are working\n  - Create feature branch: `git checkout -b feature/unified-query-command`\n  - Run existing tests to ensure baseline: `./tests/run_all_tests.sh`\n\n  ## Phase 2: Implementation Planning\n  [Detailed breakdown based on feature requirements]\n\n  1. **Refactor existing query.py command**\n     - Keep existing comment-specific functionality as legacy support\n     - Extract common query patterns into reusable components\n     - File: `airules/cli/commands/query.py`\n\n  2. **Create new unified query system**\n     - New class: `UnifiedQueryBuilder` to handle all entity types\n     - New class: `EntityTraverser` for hierarchical data retrieval\n     - New class: `UnifiedResultsHandler` for consistent display\n     - Location: Keep in `airules/cli/commands/query.py` to maintain structure",
    "execution_notes": "4. Understand the testing philosophy and approach\n  5. Document everything needed for successful implementation\n\n  ## Phase 1: Project Structure Deep Dive\n  [Specific areas to investigate based on the feature]\n  - CLI command organization in `airules/cli/commands/`\n  - Delta Spark models in `airules/delta_spark/models/`\n  - Table registry and schema management in `airules/delta_spark/`\n  - Current query command implementation in `airules/cli/commands/query.py`\n  - Infrastructure commands in `airules/cli/commands/infrastructure.py` for display patterns\n\n  ## Phase 2: Architecture Analysis\n  [What architectural aspects to focus on]\n  - Click framework patterns with AppContext passing\n  - Delta Lake integration with AbstractDeltaModel base class\n  - Entity relationships: Docket \u2192 Documents \u2192 Comments \u2192 Attachments\n  - QueryBuilder and QueryResultsHandler patterns in current query command\n  - DeltaTableRegistry for table management\n  - Configuration system in `airules/config.py`\n  - Spark session management in `airules/utils/spark_session.py`\n\n  ## Phase 3: Code Standards Discovery\n  [Specific patterns to document]\n  - Command naming: verb-noun pattern (e.g., list-tables, show-config)\n  - CLI decorator pattern: @click.command() with @pass_context\n  - Error handling: try/except with click.echo for user messages\n  - Progress bars: tqdm for long operations\n  - Output formatting: support for table (default), json, csv\n  - Class naming: PascalCase with descriptive suffixes (Handler, Builder, Manager)\n  - Method naming: snake_case with clear action verbs\n  - Import organization: standard library, third-party, local imports\n\n  ## Phase 4: Feature-Specific Investigation\n  [Targeted analysis for this feature]\n  - Current query command limitations (only queries comments)\n  - Entity model methods to leverage:\n    - Docket.get_by_id(), Document.get_by_docket()\n    - Comment.get_by_document(), Attachment.get_by_comment()\n    - Comment.get_comments_with_attachments()\n  - Display patterns from infrastructure commands:\n    - Table formatting with headers and separators\n    - JSON output with proper serialization\n    - CSV export capabilities\n  - Configurable switches pattern from existing commands (--format, --output-file)\n  - Entity type detection from table names in DeltaTableRegistry\n\n  ## Phase 5: Testing Infrastructure\n  [Testing approach analysis]\n  - Test organization in `tests/unit/` and `tests/integration/`\n  - Mock patterns using unittest.mock\n  - Test CLI commands with Click's CliRunner\n  - Test data fixtures and sample docket IDs\n  - Delta table testing with temporary tables\n  - Coverage expectations from existing test suites\n\n  ## Expected Deliverable\n  A comprehensive analysis document that allows someone to implement Refactor the query CLI function to return entities against their route. so if its query docket\n  {docket}, it returns the docket and all of its documents, comments, attachments, etc, with configurable switches. without needing to ask any questions about the codebase.\n\n  Key findings should include:\n  - Exact patterns for CLI command implementation\n  - Entity traversal logic for hierarchical data retrieval\n  - Display formatting patterns to maintain consistency\n  - Configuration and switch implementation patterns\n  - Testing patterns for CLI commands with Delta Lake integration\n\n\u25cf Update Todos\n  \u23bf \u00a0\u2612 Create detailed analysis template for understanding AIRules architecture\n     \u2610 Create implementation template for refactoring query command\n     \u2610 Create execution notes with risk assessment and estimates\n\n\u25cf IMPLEMENTATION_TEMPLATE\n\n  # Role: Senior Software Engineer\n\n  You are a senior software engineer with deep expertise in building production-ready features. You have:\n  - 15+ years of experience in software development\n  - Expertise in maintaining code quality and consistency\n  - Strong focus on testing and documentation\n  - Ability to implement complex features while following existing patterns perfectly\n\n  ## Your Mission\n  Implement the feature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its\n  documents, comments, attachments, etc, with configurable switches.\n\n  You must follow the patterns and conventions discovered during the analysis phase EXACTLY. No improvements or refactoring - pure feature implementation following existing\n   patterns.\n\n  ## Implementation Constraints\n  - Follow existing patterns precisely\n  - Minimize changes to existing code\n  - Write comprehensive tests\n  - Document all decisions\n  - Handle errors consistently with the project\n  - Maintain backward compatibility\n\n  ## Phase 1: Pre-Implementation Setup\n  - Review analysis findings\n  - Set up development environment with AIRules\n  - Verify Spark and Delta Lake are working\n  - Create feature branch: `git checkout -b feature/unified-query-command`\n  - Run existing tests to ensure baseline: `./tests/run_all_tests.sh`\n\n  ## Phase 2: Implementation Planning\n  [Detailed breakdown based on feature requirements]\n\n  1. **Refactor existing query.py command**\n     - Keep existing comment-specific functionality as legacy support\n     - Extract common query patterns into reusable components\n     - File: `airules/cli/commands/query.py`\n\n  2. **Create new unified query system**\n     - New class: `UnifiedQueryBuilder` to handle all entity types\n     - New class: `EntityTraverser` for hierarchical data retrieval\n     - New class: `UnifiedResultsHandler` for consistent display\n     - Location: Keep in `airules/cli/commands/query.py` to maintain structure\n\n  3. **Add configurable switches**\n     - `--include-documents`: Include documents when querying docket\n     - `--include-comments`: Include comments (implies --include-documents)\n     - `--include-attachments`: Include attachments (implies --include-comments)\n     - `--depth`: Control traversal depth (0=entity only, -1=all levels)\n     - `--format`: Existing pattern for output format\n     - `--output-file`: Existing pattern for file export\n\n  4. **Entity route detection**\n     - Parse input to detect entity type: \"docket:ID\", \"document:ID\", \"comment:ID\", \"attachment:ID\"\n     - Fall back to auto-detection by trying each entity type\n     - Use DeltaTableRegistry to validate entity types\n\n  ## Phase 3: Core Implementation\n  [Step-by-step implementation guide]\n\n  1. **Refactor QueryFilter for multi-entity support**\n     ```python\n     @dataclass\n     class UnifiedQueryFilter:\n         entity_type: str  # 'docket', 'document', 'comment', 'attachment'\n         entity_id: str\n         include_documents: bool = False\n         include_comments: bool = False\n         include_attachments: bool = False\n         depth: int = 0\n         # ... other existing filters\n\n  2. Implement EntityTraverser class\n  class EntityTraverser:\n      def __init__(self, ctx: AppContext):\n          self.ctx = ctx\n          self.schema_manager = ctx.get_schema_manager()\n\n      def traverse_hierarchy(self, entity_type: str, entity_id: str,\n                           filter: UnifiedQueryFilter) -> Dict[str, List]:\n          \"\"\"Traverse entity hierarchy based on filter settings\"\"\"\n          # Implementation following existing get_by_* patterns\n  3. Extend QueryBuilder for multi-entity SQL\n  class UnifiedQueryBuilder(QueryBuilder):\n      def build_entity_query(self, entity_type: str, filter: UnifiedQueryFilter) -> str:\n          \"\"\"Build SQL query for any entity type\"\"\"\n          # Use DeltaTableRegistry to get table info\n          # Follow existing SQL patterns\n  4. Create UnifiedResultsHandler\n  class UnifiedResultsHandler(QueryResultsHandler):\n      def display_hierarchical_results(self, results: Dict[str, List],\n                                     format: str, output_file: Optional[str]):\n          \"\"\"Display results maintaining hierarchy\"\"\"\n          # Follow infrastructure.py display patterns\n  5. Update query command\n  @click.command()\n  @click.argument('entity_spec')  # Format: \"docket:EPA-HQ-OAR-2023-0072\" or just ID\n  @click.option('--include-documents', is_flag=True, help='Include documents')\n  @click.option('--include-comments', is_flag=True, help='Include comments (implies documents)')\n  @click.option('--include-attachments', is_flag=True, help='Include attachments (implies comments)')\n  @click.option('--depth', type=int, default=0, help='Traversal depth (-1 for all)')\n  @click.option('--format', type=click.Choice(['table', 'json', 'csv']), default='table')\n  @click.option('--output-file', help='Export results to file')\n  @pass_context\n  def query(ctx, entity_spec, include_documents, include_comments,\n           include_attachments, depth, format, output_file):\n      \"\"\"Query entities with hierarchical traversal\"\"\"\n      # Implementation following existing patterns\n\n  Phase 4: Testing Implementation\n\n  [Comprehensive testing requirements]\n\n  1. Unit tests for new components\n    - File: tests/unit/test_unified_query.py\n    - Test UnifiedQueryFilter validation\n    - Test EntityTraverser with mocked Delta models\n    - Test UnifiedQueryBuilder SQL generation\n    - Test entity type detection logic\n  2. Integration tests\n    - File: tests/integration/test_query_command.py\n    - Test full hierarchy traversal\n    - Test each entity type as entry point\n    - Test depth control and include flags\n    - Test output formats\n  3. CLI tests\n    - Update tests/test_cli.sh with new query examples\n    - Test backward compatibility with existing query syntax\n    - Test error handling for invalid entities\n  4. Performance tests\n    - Test with large datasets using test docket\n    - Verify memory usage with deep traversal\n    - Test query optimization\n\n  Phase 5: Documentation and Polish\n\n  - Update docstrings following existing patterns\n  - Add examples to query command help text\n  - Update CLAUDE.md with new query syntax examples\n  - Create migration notes for users of old query command\n  - Update README.md if it exists\n\n  Expected Deliverable\n\n  Complete, working implementation of Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all\n   of its documents, comments, attachments, etc, with configurable switches. that:\n  - Maintains full backward compatibility\n  - Follows all existing Click and Delta Lake patterns\n  - Provides intuitive entity traversal with configurable depth\n  - Handles all edge cases (missing entities, permissions, etc.)\n  - Includes comprehensive test coverage\n  - Integrates seamlessly with existing CLI structure\n\n\u25cf Update Todos\n  \u23bf \u00a0\u2612 Create detailed analysis template for understanding AIRules architecture\n     \u2612 Create implementation template for refactoring query command\n     \u2610 Create execution notes with risk assessment and estimates\n\n\u25cf EXECUTION_NOTES\n\n  Feature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its documents,\n  comments, attachments, etc, with configurable switches.\n  Complexity: Medium-High\n  Estimated Time: 16-20 hours broken down by phase\n    - Analysis: 2-3 hours\n    - Implementation: 8-10 hours\n    - Testing: 4-5 hours\n    - Documentation: 2 hours\n\n  Key Risks:\n  1. **Performance Impact** (Medium Risk)\n     - Deep traversal of large dockets could be slow\n     - Mitigation: Implement lazy loading, pagination options, and depth limits\n\n  2. **Backward Compatibility** (Low Risk)\n     - Existing query command usage must continue working\n     - Mitigation: Keep existing functionality, add new features as extensions\n\n  3. **Memory Usage** (Medium Risk)\n     - Loading full hierarchy could exhaust memory\n     - Mitigation: Stream results, implement batch processing\n\n  4. **Delta Lake Query Complexity** (Low Risk)\n     - Complex joins across multiple tables\n     - Mitigation: Use existing model methods, optimize with z-ordering\n\n  Key Files to Focus On:\n  - `airules/cli/commands/query.py` - Main implementation file\n  - `airules/delta_spark/models/*.py` - Entity models to understand relationships\n  - `airules/delta_spark/table_registry.py` - Table configurations\n  - `airules/cli/commands/infrastructure.py` - Display patterns reference\n  - `tests/unit/test_query_command.py` - Existing test patterns\n\n  Similar Existing Features:\n  - `list-tables` command - Shows hierarchical grouping and formatting\n  - `stats` command - Shows detailed entity information\n  - Current `query` command - Base implementation to extend\n  - `get_comments_with_attachments()` - Example of joined data retrieval\n\n  Integration Points:\n  - AppContext for accessing schema manager and Spark session\n  - DeltaTableRegistry for entity type validation\n  - Entity models (Docket, Document, Comment, Attachment) for data access\n  - Click decorators and context passing\n  - Output formatting utilities\n\n  Testing Strategy:\n  - Unit test each new component in isolation\n  - Integration test with test docket EPA-HQ-OAR-2023-0072\n  - Use CliRunner for CLI testing\n  - Mock Spark/Delta operations for fast unit tests\n  - Test all combinations of include flags and depth settings\n  - Verify output formats (table, JSON, CSV)\n\n  Performance Considerations:\n  - Use Spark's lazy evaluation where possible\n  - Implement result pagination for large datasets\n  - Consider caching for repeated queries\n  - Use Delta Lake's data skipping for query optimization\n  - Monitor memory usage during deep traversal\n\n  Security Considerations:\n  - Validate entity IDs to prevent injection\n  - Respect existing access controls (if any)\n  - Sanitize output for CSV/JSON export\n  - Log access for audit purposes\n\n  Rollback Plan:\n  1. Keep existing query command functional during development\n  2. Use feature flag or separate command name initially\n  3. Git branch protection until feature is stable\n  4. Comprehensive test suite before merging\n  5. Document rollback procedure in deployment notes\n  6. Keep old command available as `query-legacy` if needed\n\n  Additional Notes:\n  - Consider adding --limit flag for result pagination\n  - Future enhancement: parallel fetching for better performance\n  - Consider progress bars for long-running queries\n  - May want to add --summary flag for counts only\n  - Could add --export-format for different file formats",
    "analysis_output": "  \u25cf IMPLEMENTATION_TEMPLATE\n\n    # Role: Senior Software Engineer\n\n    You are a senior software engineer with deep expertise in building production-ready features. You have:\n    - 15+ years of experience in software development\n    - Expertise in maintaining code quality and consistency\n    - Strong focus on testing and documentation\n    - Ability to implement complex features while following existing patterns perfectly\n\n    ## Your Mission\n    Implement the feature: Refactor the query CLI function to return entities against their route. so if its query docket {docket}, it returns the docket and all of its\n    documents, comments, attachments, etc, with configurable switches.\n\n    You must follow the patterns and conventions discovered during the analysis phase EXACTLY. No improvements or refactoring - pure feature implementation following\n  existing\n     patterns.\n\n    ## Implementation Constraints\n    - Follow existing patterns precisely\n    - Minimize changes to existing code\n    - Write comprehensive tests\n    - Document all decisions\n    - Handle errors consistently with the project\n    - Maintain backward compatibility\n\n    ## Phase 1: Pre-Implementation Setup\n    - Review analysis findings\n    - Set up development environment with AIRules\n    - Verify Spark and Delta Lake are working\n    - Create feature branch: `git checkout -b feature/unified-query-command`\n    - Run existing tests to ensure baseline: `./tests/run_all_tests.sh`\n\n    ## Phase 2: Implementation Planning\n    [Detailed breakdown based on feature requirements]\n\n    1. **Refactor existing query.py command**\n       - Keep existing comment-specific functionality as legacy support\n       - Extract common query patterns into reusable components\n       - File: `airules/cli/commands/query.py`\n\n  ## Expected Analysis Output\n\n  Your deliverable should include:\n\n  ### 1. Architecture Overview\n  - System design pattern (MVC, microservices, etc.)\n  - Component organization and responsibilities\n  - Data flow diagrams (described textually)\n  - Integration patterns\n\n  ### 2. Code Conventions Document\n  - Naming standards (with examples)\n  - File organization patterns\n  - Import/export conventions\n  - Error handling patterns\n  - Logging approaches\n\n  ### 3. Feature Integration Points\n  - Exactly where this feature will integrate\n  - Which files will need modification\n  - What patterns to follow for integration\n  - Potential conflicts or challenges\n\n  ### 4. Testing Strategy\n  - How similar features are tested\n  - Test file organization\n  - Mocking/stubbing patterns\n  - Expected test coverage\n\n  ### 5. Implementation Readiness Checklist\n  - Prerequisites confirmed\n  - Patterns identified\n  - Integration points mapped\n  - Risks documented\n\n  ## Critical Reminders\n\n  1. **Be Specific**: Don't say \"uses MVC pattern\" - show exactly how with file examples\n  2. **Document Locations**: Always include file paths and line numbers when relevant\n  3. **Show Examples**: Include code snippets that demonstrate patterns\n  4. **Think Integration**: Focus on how the new feature will fit into existing code\n  5. **Anticipate Issues**: Flag any potential problems or complexity\n\n  Remember: Your analysis will be the only guide for implementation. Make it comprehensive.\n\n\u273b Thinking\u2026\n\n  The user wants me to execute a deep analysis of the AIRules codebase to prepare for implementing a refactored query CLI function. I need to follow the analysis template\n  methodically, examining the codebase and documenting my findings in detail.\n\n  Let me start with Phase 1: Project Structure Deep Dive.\n\n\u25cf I'll execute a comprehensive analysis of the AIRules codebase following the template. Let me start by exploring the project structure systematically.\n\n\u25cf Task(Analyze CLI command organization)\n  \u23bf \u00a0Done (7 tool uses \u00b7 29.1k tokens \u00b7 1m 12.4s)\n\n\u273b Thinking\u2026\n\n  Good, I've got a comprehensive understanding of the CLI structure. Now let me move to analyzing the Delta Spark models and entity relationships.\n\n\u25cf Task(Analyze Delta Spark models and relationships)\n  \u23bf \u00a0Done (12 tool uses \u00b7 54.4k tokens \u00b7 2m 1.9s)\n\n\u273b Thinking\u2026\n\n  Excellent, I now have a deep understanding of the entity models and relationships. Let me continue with analyzing the current query command implementation and how it\n  works.\n\n\u25cf Task(Analyze current query command implementation)\n  \u23bf \u00a0Done (11 tool uses \u00b7 31.8k tokens \u00b7 1m 39.0s)\n\n\u273b Thinking\u2026\n\n  Good, I have a thorough understanding of the current query implementation. Now let me analyze the display patterns from infrastructure commands to understand how results\n  should be formatted in the new unified query.\n\n\u25cf Task(Analyze display patterns and testing infrastructure)\n  \u23bf \u00a0Done (15 tool uses \u00b7 36.3k tokens \u00b7 1m 49.0s)\n\n\u273b Thinking\u2026\n\n  Now I have comprehensive information about the codebase. Let me compile a final analysis document that brings together all my findings into a complete deliverable.\n\n\u25cf Task(Compile final comprehensive analysis)\n  \u23bf \u00a0Done (4 tool uses \u00b7 32.1k tokens \u00b7 4m 35.6s)\n\n\u273b Thinking\u2026\n\n  Now let me create the final comprehensive analysis document that brings together all my findings in a structured format.\n\n\u25cf Comprehensive Analysis: AIRules Query Command Refactoring\n\n  1. Architecture Overview\n\n  System Architecture\n\n  The AIRules system follows a layered architecture with clear separation of concerns:\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                    CLI Layer                            \u2502\n  \u2502  (airules/cli/__init__.py, commands/*.py)              \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502                 Business Logic Layer                    \u2502\n  \u2502  (airules/analysis/*, airules/pipelines/*)             \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502                  Data Access Layer                      \u2502\n  \u2502  (airules/delta_spark/models/*)                        \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502              Delta Lake / Spark Layer                   \u2502\n  \u2502  (airules/delta_spark/base.py, utils/spark_session.py) \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  Component Organization\n\n  - CLI Commands: /airules/cli/commands/ - Each command in separate file\n  - Data Models: /airules/delta_spark/models/ - One model per entity\n  - Configuration: /airules/config.py - Centralized config management\n  - Table Registry: /airules/delta_spark/delta_table_registry.py - Table metadata\n  - Utilities: /airules/utils/ - Shared utilities (logging, spark, config)\n\n  Data Flow Pattern\n\n  1. CLI command receives user input\n  2. Command validates and creates AppContext\n  3. AppContext provides Spark session and schema manager\n  4. Data models query Delta tables using Spark SQL\n  5. Results formatted and displayed based on output format\n\n  2. Code Conventions Document\n\n  Naming Standards\n\n  - Commands: Verb-noun pattern (list-tables, show-config)\n  - Classes: PascalCase with descriptive suffixes (QueryBuilder, ResultsHandler)\n  - Methods: snake_case with action verbs (get_by_id, build_query)\n  - Files: snake_case matching content (query.py, delta_operations.py)\n\n  File Organization Pattern\n\n  # Standard import order (from any command file)\n  import sys\n  import logging\n  from typing import List, Optional, Dict\n  from dataclasses import dataclass\n\n  import click\n  from pyspark.sql import DataFrame\n  from tqdm import tqdm\n\n  from airules.cli.common import AppContext, pass_context\n  from airules.delta_spark.models import Comment\n  from airules.utils.logging_utils import log_exception\n\n  Command Structure Pattern\n\n  @click.command()\n  @click.argument(\"entity_id\")\n  @click.option(\"--format\", type=click.Choice([\"table\", \"json\", \"csv\"]), default=\"table\")\n  @click.option(\"--limit\", type=int, default=100, help=\"Maximum results\")\n  @pass_context\n  def command_name(ctx: AppContext, entity_id: str, format: str, limit: int):\n      \"\"\"Command description for help text.\"\"\"\n      logger = logging.getLogger(__name__)\n\n      try:\n          # Initialize resources\n          spark = ctx.get_spark()\n          schema_manager = ctx.get_schema_manager()\n\n          # Validate input\n          if not entity_id:\n              raise click.BadParameter(\"Entity ID required\")\n\n          # Execute logic\n          # ...\n\n          # Display results\n          click.echo(f\"\u2713 Operation completed successfully\")\n\n      except Exception as e:\n          logger.error(f\"Failed to execute: {str(e)}\")\n          if ctx.debug:\n              log_exception(logger, e, \"command_name\")\n          sys.exit(1)\n      finally:\n          ctx.cleanup()\n\n  Error Handling Pattern\n\n  # From airules/analysis/nlp/nlp_analyzer.py\n  try:\n      # Operation\n      result = perform_operation()\n  except ValueError as e:\n      logger.error(f\"Invalid value: {str(e)}\")\n      raise click.ClickException(str(e))\n  except Exception as e:\n      logger.error(f\"Unexpected error: {str(e)}\")\n      if debug_mode:\n          log_exception(logger, e, \"operation_name\")\n      raise\n\n  Delta Table Access Pattern\n\n  # From airules/delta_spark/models/comments.py\n  class Comment(AbstractDeltaModel):\n      def get_by_docket(self, docket_id: str) -> DataFrame:\n          \"\"\"Get all comments for a docket.\"\"\"\n          table_name = self._get_qualified_table_name()\n          query = f\"\"\"\n          SELECT * FROM {table_name}\n          WHERE docket_id = '{docket_id}'\n          ORDER BY posted_date DESC\n          \"\"\"\n          return self.spark.sql(query)\n\n  3. Feature Integration Points\n\n  Primary Integration Points\n\n  1. CLI Command Registration (airules/cli/init.py:173)\n  # Add after existing commands\n  cli.add_command(query)  # Enhanced unified query\n  2. Query Command Enhancement (airules/cli/commands/query.py)\n    - Extend existing QueryFilter dataclass\n    - Enhance QueryBuilder for multi-table support\n    - Update QueryResultsHandler for hierarchical display\n  3. Entity Model Integration\n    - Use existing methods: Docket.get_by_id(), Document.get_by_docket()\n    - Leverage Comment.get_comments_with_attachments() for joins\n    - Utilize Attachment.get_all_by_docket() with include_comment_columns\n  4. Table Registry Usage (airules/delta_spark/delta_table_registry.py:120)\n  # Get table info for dynamic queries\n  table_config = registry.get_table(\"comments\")\n  schema = table_config.schema\n  5. Display Integration (Following infrastructure.py patterns)\n  # Hierarchical display\n  click.echo(\"DOCKET INFORMATION:\")\n  click.echo(\"-\" * 50)\n  click.echo(f\"  \ud83d\udcca {docket_id}\")\n  click.echo(f\"     Title: {title}\")\n  click.echo()\n  click.echo(\"DOCUMENTS:\")\n  # ... continue hierarchy\n\n  Configuration Points\n\n  - Environment detection: ctx.environment (local/databricks)\n  - Debug mode: ctx.debug for verbose output\n  - Config overrides: ctx.config for custom settings\n\n  4. Testing Strategy\n\n  Test Organization\n\n  tests/\n  \u251c\u2500\u2500 unit/\n  \u2502   \u251c\u2500\u2500 test_unified_query.py      # New test file\n  \u2502   \u251c\u2500\u2500 test_query_builder.py      # Test query building\n  \u2502   \u2514\u2500\u2500 test_entity_traverser.py   # Test traversal logic\n  \u251c\u2500\u2500 integration/\n  \u2502   \u2514\u2500\u2500 test_query_integration.py  # End-to-end tests\n  \u2514\u2500\u2500 fixtures/\n      \u2514\u2500\u2500 query_fixtures.py          # Test data\n\n  Unit Testing Pattern\n\n  # From existing test patterns\n  import unittest\n  from unittest.mock import Mock, patch\n  from click.testing import CliRunner\n\n  class TestUnifiedQuery(unittest.TestCase):\n      def setUp(self):\n          self.runner = CliRunner()\n          self.mock_ctx = Mock(spec=AppContext)\n          self.mock_spark = Mock()\n          self.mock_ctx.get_spark.return_value = self.mock_spark\n\n      def test_query_docket_basic(self):\n          # Mock data\n          mock_docket_df = Mock()\n          mock_docket_df.collect.return_value = [Row(docket_id=\"EPA-123\")]\n\n          # Execute\n          result = self.runner.invoke(\n              query,\n              ['docket:EPA-123'],\n              obj=self.mock_ctx\n          )\n\n          # Assert\n          assert result.exit_code == 0\n          assert \"EPA-123\" in result.output\n\n  Integration Testing Pattern\n\n  @pytest.mark.integration\n  def test_full_hierarchy_query(spark_session, temp_delta_path):\n      \"\"\"Test querying full entity hierarchy.\"\"\"\n      # Setup test data\n      create_test_docket(spark_session, temp_delta_path)\n\n      # Execute query\n      result = query_entities(\n          \"docket:TEST-123\",\n          include_all=True,\n          spark=spark_session\n      )\n\n      # Verify hierarchy\n      assert len(result[\"documents\"]) > 0\n      assert len(result[\"comments\"]) > 0\n      assert len(result[\"attachments\"]) > 0\n\n  Mock Patterns\n\n  - Mock Spark DataFrames: Return Mock() with .collect() method\n  - Mock Schema Manager: Return table existence checks\n  - Mock File I/O: Use StringIO for CSV exports\n  - Mock Progress Bars: Disable in tests with disable=True\n\n  5. Implementation Readiness Checklist\n\n  Prerequisites Confirmed \u2713\n\n  - CLI command structure understood (Click framework)\n  - Delta Lake model patterns documented\n  - Entity relationships mapped\n  - Display formatting patterns identified\n  - Testing patterns analyzed\n\n  Patterns Identified \u2713\n\n  - Command registration pattern in cli/init.py\n  - AppContext usage for dependency injection\n  - SQL query building with proper escaping\n  - Multi-format output support (table/json/csv)\n  - Error handling and logging conventions\n\n  Integration Points Mapped \u2713\n\n  - Where to add new command: /airules/cli/commands/query.py\n  - How to access entities: Through model classes in /airules/delta_spark/models/\n  - Display formatting: Follow infrastructure.py patterns\n  - Configuration: Use AppContext and config.py\n  - Testing: Add to /tests/unit/ and /tests/integration/\n\n  Risks Documented \u2713\n\n  - Performance with deep traversal (Mitigation: pagination, limits)\n  - Memory usage with large results (Mitigation: streaming, batching)\n  - Complex SQL generation (Mitigation: use existing patterns)\n  - Backward compatibility (Mitigation: extend, don't replace)\n\n  Implementation Approach \u2713\n\n  1. Phase 1: Extend QueryFilter for multi-entity support\n  2. Phase 2: Create EntityTraverser using existing model methods\n  3. Phase 3: Enhance QueryBuilder for dynamic joins\n  4. Phase 4: Update ResultsHandler for hierarchical display\n  5. Phase 5: Add comprehensive tests following existing patterns\n\n  Key Takeaways\n\n  The AIRules codebase is well-structured with clear patterns that make extending the query functionality straightforward:\n\n  1. Use existing infrastructure: The models already have the methods needed\n  2. Follow established patterns: Command structure, error handling, and display formats are consistent\n  3. Leverage the type system: Delta tables have schemas that can guide implementation\n  4. Test comprehensively: The existing test patterns show how to mock Spark operations effectively\n  5. Maintain compatibility: Extend the existing query command rather than replacing it"
  },
  "complexity": "Medium"
}